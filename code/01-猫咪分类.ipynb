{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "base_path = '../dataset/01-猫咪分类/data/' \n",
    "dataset_path = '../dataset/01-猫咪分类/data/cat_12_train'\n",
    "annotation_file = '../dataset/01-猫咪分类/data/train_list.txt'\n",
    "\n",
    "def read_annotation_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for line in lines:\n",
    "        path, label = line.strip().split('\\t')\n",
    "        full_path = os.path.join(base_path, path)\n",
    "        image_paths.append(full_path)\n",
    "        labels.append(int(label))\n",
    "    return image_paths, labels\n",
    "\n",
    "image_paths, labels = read_annotation_file(annotation_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建数据集和加载器\n",
    "使用transform进行数据增广"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练集的图像变换，包括数据增广操作\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # 随机水平翻转\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0),  # 改变图像的亮度等\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 使用ImageNet的均值和标准差进行归一化\n",
    "])\n",
    "\n",
    "# 定义测试集的图像变换\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # 将图像转换为PyTorch张量\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 使用ImageNet的均值和标准差进行归一化\n",
    "])\n",
    "\n",
    "# 随机分割图像路径和标签\n",
    "train_image_paths, test_image_paths, train_labels, test_labels = train_test_split(\n",
    "    image_paths, labels, test_size=0.1, stratify=labels)\n",
    "\n",
    "# 创建训练集和测试集\n",
    "train_dataset = CustomImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
    "test_dataset = CustomImageDataset(test_image_paths, test_labels, transform=test_transform)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过打印数据集中的图片和标签来验证数据集是否加载成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 获取一批数据\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# 取出第一张图片和它的标签\n",
    "image, label = images[0], labels[0]\n",
    "\n",
    "# 因为图像被转换为了 Tensor，并且进行了标准化，所以我们需要先将其转换回 PIL Image\n",
    "image = image.permute(1, 2, 0)  \n",
    "image = image * torch.Tensor([0.229, 0.224, 0.225]) + torch.Tensor([0.485, 0.456, 0.406])  \n",
    "image = (image * 255).byte()  \n",
    "\n",
    "# 显示图片\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# 打印标签\n",
    "print(\"Label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建CNN网络(RESNET50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "\n",
    "    # 将resnet网络的输出通道改为12\n",
    "    def __init__(self, num_classes=12):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(Bottleneck, 64, 3)\n",
    "        self.layer2 = self._make_layer(Bottleneck, 128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(Bottleneck, 256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(Bottleneck, 512, 3, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "       \n",
    "        self.fc = nn.Linear(512 * Bottleneck.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model_cnn = ResNet50()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将resnet50的预训练模型参数导入自己定义的网络中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_cnn = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# 获取预训练模型的参数\n",
    "pretrained_dict_cnn = pretrained_model_cnn.state_dict()\n",
    "\n",
    "# 获取我的模型的参数\n",
    "model_dict_cnn = model_cnn.state_dict()\n",
    "\n",
    "# 过滤出在自己模型中存在的预训练参数\n",
    "pretrained_dict_cnn = {k: v for k, v in pretrained_dict_cnn.items() if k in model_dict_cnn and model_dict_cnn[k].shape == pretrained_dict_cnn[k].shape}\n",
    "\n",
    "# 更新模型的参数\n",
    "model_dict_cnn.update(pretrained_dict_cnn)\n",
    "\n",
    "# 将更新后的参数加载到我自己的模型中\n",
    "model_cnn.load_state_dict(model_dict_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建Transformer网络（VIT）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将图像分割为小块并进行线性嵌入\n",
    "class PatchEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size, patch_size, in_channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.projection = nn.Conv2d(in_channels, hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)  # (B, hidden_dim, H/P, W/P)\n",
    "        x = x.flatten(2)        # (B, hidden_dim, num_patches)\n",
    "        x = x.transpose(1, 2)   # (B, num_patches, hidden_dim)\n",
    "        return x\n",
    "    \n",
    "#多头自注意力机制\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = hidden_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(hidden_dim, hidden_dim * 3)\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "        self.out = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attention_dropout(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.out(x)\n",
    "        x = self.out_dropout(x)\n",
    "        return x\n",
    "    \n",
    "# Transformer编码器\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads, mlp_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.attention = MultiHeadAttention(hidden_dim, num_heads, dropout_rate)\n",
    "        self.ln_2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(mlp_dim, hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, num_layers, num_heads, hidden_dim, mlp_dim, num_classes, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, hidden_dim)\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, 1 + self.patch_embedding.num_patches, hidden_dim))\n",
    "\n",
    "        self.encoder = nn.Sequential(*[Encoder(hidden_dim, num_heads, mlp_dim, dropout_rate) for _ in range(num_layers)])\n",
    "\n",
    "        self.ln = nn.LayerNorm(hidden_dim)\n",
    "        self.head = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        class_token = self.class_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "        x = x + self.position_embedding\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        return self.head(x[:, 0])\n",
    "    \n",
    "model_trans = VisionTransformer(img_size=224, patch_size=16, in_channels=3, num_layers=12, num_heads=12, hidden_dim=768, mlp_dim=3072, num_classes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将VIT的预训练模型参数导入自己定义的网络中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取预训练模型的参数\n",
    "pretrained_dict_trans = torch.load('../state/01-cat/vit_b_16-c867db91.pth')\n",
    "\n",
    "# 获取我的模型的参数\n",
    "model_dict_trans = model_trans.state_dict()\n",
    "\n",
    "# 修改在自己模型中不存在的预训练参数\n",
    "pretrained_dict_fix = {}\n",
    "for k, v in pretrained_dict_trans.items():\n",
    "    if k in model_dict_trans:\n",
    "        pretrained_dict_fix[k] = v\n",
    "    elif \"conv_proj\" in k:\n",
    "        key = k.replace(\"conv_proj\", \"patch_embedding.projection\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"encoder.pos_embedding\" in k:\n",
    "        key = k.replace(\"encoder.pos_embedding\", \"position_embedding\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"ln_1\" in k:\n",
    "        key = k.replace(\"ln_1\", \"ln_1\")\n",
    "        key = key.replace(\"encoder.layers.encoder_layer_\", \"encoder.\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"ln_2\" in k:\n",
    "        key = k.replace(\"ln_2\", \"ln_2\")\n",
    "        key = key.replace(\"encoder.layers.encoder_layer_\", \"encoder.\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"self_attention.in_proj_weight\" in k:\n",
    "        key = k.replace(\"self_attention.in_proj_weight\", \"attention.qkv.weight\")\n",
    "        key = key.replace(\"encoder.layers.encoder_layer_\", \"encoder.\")\n",
    "        key = key.replace(\"self_attention\", \"attention\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"self_attention.in_proj_bias\" in k:\n",
    "        key = k.replace(\"self_attention.in_proj_bias\", \"attention.qkv.bias\")\n",
    "        key = key.replace(\"encoder.layers.encoder_layer_\", \"encoder.\")\n",
    "        key = key.replace(\"self_attention\", \"attention\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"self_attention.out_proj.weight\" in k:\n",
    "        key = k.replace(\"self_attention.out_proj.weight\", \"attention.out.weight\")\n",
    "        key = key.replace(\"encoder.layers.encoder_layer_\", \"encoder.\")\n",
    "        key = key.replace(\"self_attention\", \"attention\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"self_attention.out_proj.bias\" in k:\n",
    "        key = k.replace(\"self_attention.out_proj.bias\", \"attention.out.bias\")\n",
    "        key = key.replace(\"encoder.layers.encoder_layer_\", \"encoder.\")\n",
    "        key = key.replace(\"self_attention\", \"attention\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"mlp.linear_1\" in k:\n",
    "        key = k.replace(\"mlp.linear_1\", \"mlp.0\")\n",
    "        key = key.replace(\"encoder.layers.encoder_layer_\", \"encoder.\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"mlp.linear_2\" in k:\n",
    "        key = k.replace(\"mlp.linear_2\", \"mlp.3\")\n",
    "        key = key.replace(\"encoder.layers.encoder_layer_\", \"encoder.\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"encoder.ln.weight\" in k:\n",
    "        key = k.replace(\"encoder.ln.weight\", \"ln.weight\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"encoder.ln.bias\" in k:\n",
    "        key = k.replace(\"encoder.ln.bias\", \"ln.bias\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"heads.head.weight\" in k:\n",
    "        key = k.replace(\"heads.head.weight\", \"head.weight\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    elif \"heads.head.bias\" in k:\n",
    "        key = k.replace(\"heads.head.bias\", \"head.bias\")\n",
    "        pretrained_dict_fix[key] = v\n",
    "    else:\n",
    "        print(k)\n",
    "        print(\"Not found\")\n",
    "        print()\n",
    "\n",
    "# 更新模型的参数\n",
    "model_dict_trans.update(pretrained_dict_fix)\n",
    "\n",
    "\n",
    "# 将更新后的参数加载到我自己的模型中\n",
    "model_trans.load_state_dict(model_dict_trans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数及优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  在将模型和损失函数移动到GPU之前，先定义它们。这样可以避免在CPU上创建不必要的副本\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device.type}')\n",
    "\n",
    "# 选择要训练的模型(CNN或Transformer)\n",
    "model_name = 'CNN'\n",
    "\n",
    "if model_name == 'CNN':\n",
    "    model = model_cnn.to(device)\n",
    "\n",
    "    epochs = 30\n",
    "    learning_rate = 0.000025 #CNN网络推荐使用0.000025，Transformer网络推荐使用0.00002\n",
    "else:\n",
    "    model = model_trans.to(device)\n",
    "\n",
    "    epochs = 50\n",
    "    learning_rate = 0.00002\n",
    "    \n",
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "min_loss = float('inf')\n",
    "      \n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # 使用 tqdm 显示进度条，方便观察进度\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Epoch: %d\" % (epoch+1))\n",
    "    for i, data in progress_bar:\n",
    "        inputs, labels =  data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # 在进度条上实时显示当前的平均损失和准确率\n",
    "        progress_bar.set_postfix({'loss': running_loss / (i + 1), 'acc': f'{100. * correct / total:.2f}%'})\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "\n",
    "    if epoch_loss < min_loss and epoch_acc > 95:\n",
    "        min_loss = epoch_loss\n",
    "        if model_name == 'CNN':\n",
    "            torch.save(model.state_dict(), '../state/01-cat/model_cnn.pt')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), '../state/01-cat/model_trans.pt')\n",
    "\n",
    "    # 每个epoch结束后清除GPU缓存\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'CNN':\n",
    "    model.load_state_dict(torch.load('../state/01-cat/model_cnn.pt'))\n",
    "else:\n",
    "    model.load_state_dict(torch.load('../state/01-cat/model_trans.pt'))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "running_loss = 0.0\n",
    "progress_bar = tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Testing\")\n",
    "with torch.no_grad():\n",
    "    for i, data in progress_bar:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        progress_bar.set_postfix({'loss': running_loss / (i + 1), 'acc': f'{100. * correct / total:.2f}%'})\n",
    "\n",
    "print('Accuracy of the network on test images: %.2f %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "# 从测试集中随机选择3个样本\n",
    "indices = random.sample(range(len(test_dataset)), 3)\n",
    "samples = [test_dataset[i] for i in indices]\n",
    "images = torch.stack([s[0] for s in samples])\n",
    "labels = torch.tensor([s[1] for s in samples])\n",
    "\n",
    "outputs = model(images.to(device))\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "predicted = predicted.to('cpu')\n",
    "labels = labels.to('cpu')\n",
    "\n",
    "# 展示图像和预测结果\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(9, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    image = images[i].permute(1, 2, 0)\n",
    "    image = (image - image.min()) / (image.max() - image.min())  # 归一化到[0, 1]范围\n",
    "    ax.imshow(image)\n",
    "    ax.title.set_text(f'Predicted: {predicted[i]}, Truth: {labels[i]}')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
